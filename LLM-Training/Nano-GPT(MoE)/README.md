In this folder I will be including a notebook where I will be building GPT-2 from scratch and modifying it's architecture to include Mixture of Experts(MoE) 
from the "A Review of Sparse Expert Models in Deep Learning research" research paper by (https://arxiv.org/abs/2209.01667) William Fedus, Jeff Dean, Barret Zoph.
