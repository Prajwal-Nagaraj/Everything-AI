{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Nano-GPT(MOE)\n",
        "\n",
        "In this notebook I will be building GPT-2 from scratch and modifying it's architecture to include Mixture of Experts(MoE). I will also be training the model on custom dataset of the game show Jeopardy. The GPT-2 part of this notebook is a direct Implementation of Andrej Karpathy's 'Let's reproduce GPT-2 (124M)' video and I will be building on top of this by including the MoE architecture which is from the A Review of Sparse Expert Models in Deep Learning research [paper](https://arxiv.org/abs/2209.01667). I will be building the MoE architecture after having applied all the optimisations to the GPT-2 model. I will not be covering all the optimisations that andrej covers in his video as I've already done that in my previous GPT-2 Notebook.\n",
        "\n",
        "In this MoE architecture, the model will have a number of \"expert\" feedforward networks alongside the standard transformer layers.  An additional \"gating\" mechanism will determine which experts are best suited to process different parts of the input, thus allowing the model to specialize and potentially improve performance on complex tasks like Jeopardy question answering. This approach aims to make the model more efficient and scalable by activating only the necessary experts for a given input, while still maintaining the power of a large model."
      ],
      "metadata": {
        "id": "AVEJWVobIqtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "d0ACzKcrLROs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoCTG9AszWo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2087480-9b8c-4a0a-fb0c-ec65feda8e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Data\n",
        "I will be loading the training data from my drive which will also be available on the GitHub repo."
      ],
      "metadata": {
        "id": "4bbcnWM6LUOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1dl6uVTAV_j",
        "outputId": "a29f87b0-dd5f-4510-ea5d-0f6193a3576a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/Train Data/JEOPARDY_QUESTIONS1.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "formatted_output = \"\"\n",
        "\n",
        "for entry in data:\n",
        "    formatted_output += f\"Question:\\n{entry['question']}\\n\\n\"\n",
        "    formatted_output += f\"Value:\\n{entry['value']}\\n\\n\"\n",
        "    formatted_output += f\"Answer:\\n{entry['answer']}\\n\\n\"\n"
      ],
      "metadata": {
        "id": "yrRAG5OpAW3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the first few lines of the training data:"
      ],
      "metadata": {
        "id": "1HfAes3XLnbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_output[:297])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX1f-zcgLnN_",
        "outputId": "e965a0ca-ac0b-4030-80da-acfc72eba724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'\n",
            "\n",
            "Value:\n",
            "$200\n",
            "\n",
            "Answer:\n",
            "Copernicus\n",
            "\n",
            "Question:\n",
            "'No. 2: 1912 Olympian; football star at Carlisle Indian School; 6 MLB seasons with the Reds, Giants & Braves'\n",
            "\n",
            "Value:\n",
            "$200\n",
            "\n",
            "Answer:\n",
            "Jim Thorpe\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-2(MoE) Initialisation\n",
        "I will again be initialising the model with random weights that follow the normal distribution with a standard deviation of 0.02 and also to control the growth of activations in the residual path I will be scaling down the initialisations to 1/sqrt(Number of residual paths) just like how OpenAI Initialised gpt-2. Along with the usual modules, I will be including a new separate class for the MoE class which has the experts and the gates to those experts and the MLP layer will be added to every experts in the architecture.\n",
        "\n",
        "For demonstration purposes I will be implementing the model with two experts."
      ],
      "metadata": {
        "id": "sHZ_AHR2aWe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)                       # key, query, value projections for all heads, but in a batch\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)                           # output projection\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1                                              # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()                                                      # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        qkv = self.c_attn(x)                                                    # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)         # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)         # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)         # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)             # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)                        # re-assemble all head outputs side by side\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class ExpertMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class MoE(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_experts = config.num_experts\n",
        "        self.experts = nn.ModuleList([ExpertMLP(config) for _ in range(self.num_experts)])\n",
        "        self.gate = nn.Linear(config.n_embd, self.num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate_scores = F.softmax(self.gate(x), dim=-1)  # (B, T, num_experts)\n",
        "        # prepare for broadcasting\n",
        "        gate_scores = gate_scores.unsqueeze(2)  # (B, T, 1, num_experts)\n",
        "\n",
        "        # compute expert outputs\n",
        "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=-1)  # (B, T, n_embd, num_experts)\n",
        "\n",
        "        # weighted sum of experts\n",
        "        output = torch.sum(gate_scores * expert_outputs, dim=-1)  # (B, T, n_embd)\n",
        "        return output\n",
        "\n",
        "class Block(nn.Module):\n",
        "                                                                                # A block to combine all the previous layers into one block along with residual pathways for the outputs\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.moe = MoE(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.moe(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rVquYj7_DReP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length                                                              # Configuration of the GPT\n",
        "    vocab_size: int = 50304 # Changing the prior value to a value that is closer to the power of 2 (2^19=50304)\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "    num_experts: int = 2 # number of experts for the MoE layer\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config                                                    # The GPT itself with the same weights for token embeddings and the language modelling head\n",
        "                                                                                # to save some space just like in GPT-2\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)                                          # Initialising the weights\n",
        "\n",
        "    def _init_weights(self, module):                                            # A small function to initialise the weights with normal distribution and a standard deviation of\n",
        "        if isinstance(module, nn.Linear):                                       # 1/sqrt(number of residual pathways) If the module is a linear layer\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):                                  #else initialising the weights with normal distribution and a standard deviation of 0.02 If the module is an embedding layer\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
        "        # start with all of the candidate parameters (that require grad)                            # A function to decay weights of paremeters that are 2 dimensional\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and 'cuda' in device\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "                                                                                # A class to create batches for training, it uses tiktoken library for encoding and decoding\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(formatted_output)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f\"loaded {len(self.tokens)} tokens\")\n",
        "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):                                                        # A simple function that creates BxT tokens of inputs X and targets Y\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "eiDbD7qxai5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop\n",
        "The same training loop used in the previous notebook works with the modified architecture but I will onyl be training the new architecture for 100 steps as I do not have a lot of compute units remaining in my colab account."
      ],
      "metadata": {
        "id": "CCJGA-sAQbdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tiktoken                                                                  # The training loop\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"                                                                # Checking if the device is cuda or cpu\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "torch.manual_seed(452)                                                            # Setting the seed for reproducibility, but I will change the seed from the repo for experimentation\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(452)\n",
        "\n",
        "train_loader = DataLoaderLite(B=8, T=512)                                         # Initialising the data loader with batch and time variables\n",
        "\n",
        "total_batch_size = 524288\n",
        "B=8\n",
        "T=512\n",
        "assert total_batch_size % (B * T) == 0\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\"batch size per step: {grad_accum_steps}\")\n",
        "\n",
        "\n",
        "model = GPT(GPTConfig())                        # Initialising the GPT model with Flash Attention, better model config parameters,\n",
        "model.to(device)                                                                # gradient clipping and learning rate scheduler\n",
        "model = torch.compile(model)                                                    # and compiling the model\n",
        "\n",
        "\n",
        "max_lr = 3e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 50\n",
        "total_steps = 100                                                               # Although 283 steps cover the entire dataset, I will only be running it for 100 steps just as a demonstration\n",
        "\n",
        "def get_lr(it):\n",
        "  if it<warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps\n",
        "  if it>total_steps:\n",
        "    return min_lr\n",
        "  decay_ratio = (it-warmup_steps)/(total_steps-warmup_steps)\n",
        "  assert 0<= decay_ratio <=1\n",
        "  coeff = 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
        "                                                                                # Along with a new optimizer that decays multi dimensional weights\n",
        "for i in range(total_steps):\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "      x, y = train_loader.next_batch()\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "        logits, loss = model(x, y)\n",
        "      loss = loss/grad_accum_steps\n",
        "      loss_accum += loss.detach()\n",
        "      loss.backward()\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    lr = get_lr(i)\n",
        "    for param_group in optimizer.param_groups:\n",
        "      param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = (t1 - t0)*1000 # time difference in miliseconds\n",
        "    tokens_per_sec = (train_loader.B * train_loader.T * grad_accum_steps) / (t1 - t0)\n",
        "    print(f\"step {i}, loss: {loss_accum.item():.6f}, lr:{lr:.4f}, norm:{norm:.4f}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeRLw5D7cfAs",
        "outputId": "bbf9d966-8007-4ea2-914a-019a1bb1107f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 9421129 tokens\n",
            "1 epoch = 2300 batches\n",
            "total desired batch size: 524288\n",
            "batch size per step: 128\n",
            "num decayed parameter tensors: 86, with 180,996,096 parameters\n",
            "num non-decayed parameter tensors: 134, with 167,448 parameters\n",
            "using fused AdamW: True\n",
            "step 0, loss: 10.921736, lr:0.0000, norm:48.4896, dt: 54528.66ms, tok/sec: 9614.91\n",
            "step 1, loss: 9.853539, lr:0.0000, norm:32.0573, dt: 22045.51ms, tok/sec: 23782.07\n",
            "step 2, loss: 8.954621, lr:0.0000, norm:17.4170, dt: 21745.75ms, tok/sec: 24109.91\n",
            "step 3, loss: 8.580629, lr:0.0000, norm:16.1997, dt: 21517.05ms, tok/sec: 24366.17\n",
            "step 4, loss: 8.431042, lr:0.0000, norm:18.8099, dt: 21534.42ms, tok/sec: 24346.51\n",
            "step 5, loss: 8.134541, lr:0.0000, norm:8.0487, dt: 21648.03ms, tok/sec: 24218.74\n",
            "step 6, loss: 8.054600, lr:0.0000, norm:10.1629, dt: 21714.97ms, tok/sec: 24144.08\n",
            "step 7, loss: 7.872296, lr:0.0000, norm:4.8981, dt: 21694.23ms, tok/sec: 24167.17\n",
            "step 8, loss: 7.735089, lr:0.0001, norm:7.3928, dt: 21656.30ms, tok/sec: 24209.49\n",
            "step 9, loss: 7.531741, lr:0.0001, norm:7.6760, dt: 21624.44ms, tok/sec: 24245.16\n",
            "step 10, loss: 7.387359, lr:0.0001, norm:15.8270, dt: 21728.93ms, tok/sec: 24128.57\n",
            "step 11, loss: 7.200285, lr:0.0001, norm:5.6419, dt: 21745.63ms, tok/sec: 24110.04\n",
            "step 12, loss: 7.136856, lr:0.0001, norm:7.5811, dt: 21752.90ms, tok/sec: 24101.98\n",
            "step 13, loss: 6.990671, lr:0.0001, norm:2.8275, dt: 21720.30ms, tok/sec: 24138.15\n",
            "step 14, loss: 6.890388, lr:0.0001, norm:7.0683, dt: 21711.41ms, tok/sec: 24148.04\n",
            "step 15, loss: 6.877836, lr:0.0001, norm:5.3539, dt: 21746.98ms, tok/sec: 24108.55\n",
            "step 16, loss: 6.745057, lr:0.0001, norm:2.7253, dt: 21722.71ms, tok/sec: 24135.48\n",
            "step 17, loss: 6.662398, lr:0.0001, norm:4.2055, dt: 21717.86ms, tok/sec: 24140.87\n",
            "step 18, loss: 6.569036, lr:0.0001, norm:3.1673, dt: 21754.77ms, tok/sec: 24099.91\n",
            "step 19, loss: 6.489233, lr:0.0001, norm:3.0439, dt: 21720.87ms, tok/sec: 24137.52\n",
            "step 20, loss: 6.408140, lr:0.0001, norm:3.7540, dt: 21705.83ms, tok/sec: 24154.25\n",
            "step 21, loss: 6.328722, lr:0.0001, norm:2.8054, dt: 21706.39ms, tok/sec: 24153.63\n",
            "step 22, loss: 6.208107, lr:0.0001, norm:1.9585, dt: 21728.12ms, tok/sec: 24129.46\n",
            "step 23, loss: 6.108525, lr:0.0001, norm:2.2982, dt: 21731.62ms, tok/sec: 24125.58\n",
            "step 24, loss: 6.000483, lr:0.0001, norm:2.6403, dt: 21693.61ms, tok/sec: 24167.86\n",
            "step 25, loss: 5.889025, lr:0.0002, norm:2.1776, dt: 21702.51ms, tok/sec: 24157.95\n",
            "step 26, loss: 5.793322, lr:0.0002, norm:2.8744, dt: 21703.10ms, tok/sec: 24157.29\n",
            "step 27, loss: 5.699108, lr:0.0002, norm:3.1545, dt: 21732.18ms, tok/sec: 24124.96\n",
            "step 28, loss: 5.596786, lr:0.0002, norm:2.7725, dt: 21751.54ms, tok/sec: 24103.49\n",
            "step 29, loss: 5.474738, lr:0.0002, norm:2.0299, dt: 21711.05ms, tok/sec: 24148.44\n",
            "step 30, loss: 5.418307, lr:0.0002, norm:4.9781, dt: 21727.48ms, tok/sec: 24130.18\n",
            "step 31, loss: 5.335575, lr:0.0002, norm:2.4736, dt: 21689.82ms, tok/sec: 24172.08\n",
            "step 32, loss: 5.211729, lr:0.0002, norm:1.5124, dt: 21733.18ms, tok/sec: 24123.85\n",
            "step 33, loss: 5.150883, lr:0.0002, norm:2.8944, dt: 21763.85ms, tok/sec: 24089.85\n",
            "step 34, loss: 5.060507, lr:0.0002, norm:1.6942, dt: 21743.41ms, tok/sec: 24112.50\n",
            "step 35, loss: 4.999722, lr:0.0002, norm:1.6662, dt: 21730.77ms, tok/sec: 24126.53\n",
            "step 36, loss: 4.927542, lr:0.0002, norm:1.9102, dt: 21757.83ms, tok/sec: 24096.52\n",
            "step 37, loss: 4.850538, lr:0.0002, norm:1.5351, dt: 21760.06ms, tok/sec: 24094.06\n",
            "step 38, loss: 4.797238, lr:0.0002, norm:1.2162, dt: 21752.78ms, tok/sec: 24102.11\n",
            "step 39, loss: 4.757131, lr:0.0002, norm:3.0776, dt: 21726.29ms, tok/sec: 24131.50\n",
            "step 40, loss: 4.725977, lr:0.0002, norm:1.9585, dt: 21735.40ms, tok/sec: 24121.39\n",
            "step 41, loss: 4.651930, lr:0.0003, norm:1.1072, dt: 21766.58ms, tok/sec: 24086.84\n",
            "step 42, loss: 4.642423, lr:0.0003, norm:2.0716, dt: 21753.39ms, tok/sec: 24101.44\n",
            "step 43, loss: 4.666849, lr:0.0003, norm:2.7059, dt: 21750.85ms, tok/sec: 24104.25\n",
            "step 44, loss: 4.584681, lr:0.0003, norm:0.9016, dt: 21763.35ms, tok/sec: 24090.41\n",
            "step 45, loss: 4.651980, lr:0.0003, norm:3.3949, dt: 21772.06ms, tok/sec: 24080.77\n",
            "step 46, loss: 4.585545, lr:0.0003, norm:2.3102, dt: 21764.00ms, tok/sec: 24089.69\n",
            "step 47, loss: 4.526966, lr:0.0003, norm:1.4960, dt: 21750.10ms, tok/sec: 24105.09\n",
            "step 48, loss: 4.506209, lr:0.0003, norm:1.5388, dt: 21764.91ms, tok/sec: 24088.68\n",
            "step 49, loss: 4.537037, lr:0.0003, norm:4.6263, dt: 21762.37ms, tok/sec: 24091.49\n",
            "step 50, loss: 4.521162, lr:0.0003, norm:2.5670, dt: 21774.11ms, tok/sec: 24078.51\n",
            "step 51, loss: 4.461010, lr:0.0003, norm:1.7473, dt: 21761.24ms, tok/sec: 24092.74\n",
            "step 52, loss: 4.420235, lr:0.0003, norm:1.2819, dt: 21772.52ms, tok/sec: 24080.26\n",
            "step 53, loss: 4.439101, lr:0.0003, norm:2.7000, dt: 21764.46ms, tok/sec: 24089.18\n",
            "step 54, loss: 4.412541, lr:0.0003, norm:1.3617, dt: 21736.76ms, tok/sec: 24119.88\n",
            "step 55, loss: 4.378288, lr:0.0003, norm:2.1752, dt: 21755.50ms, tok/sec: 24099.10\n",
            "step 56, loss: 4.371764, lr:0.0003, norm:1.8389, dt: 21773.36ms, tok/sec: 24079.33\n",
            "step 57, loss: 4.323060, lr:0.0003, norm:1.4163, dt: 21764.28ms, tok/sec: 24089.38\n",
            "step 58, loss: 4.301161, lr:0.0003, norm:1.0479, dt: 21752.58ms, tok/sec: 24102.34\n",
            "step 59, loss: 4.272087, lr:0.0003, norm:1.3818, dt: 21772.61ms, tok/sec: 24080.17\n",
            "step 60, loss: 4.252875, lr:0.0003, norm:1.0540, dt: 21771.55ms, tok/sec: 24081.33\n",
            "step 61, loss: 4.276779, lr:0.0003, norm:1.6269, dt: 21763.46ms, tok/sec: 24090.29\n",
            "step 62, loss: 4.230551, lr:0.0003, norm:1.1882, dt: 21773.70ms, tok/sec: 24078.96\n",
            "step 63, loss: 4.267334, lr:0.0003, norm:4.3651, dt: 21771.92ms, tok/sec: 24080.93\n",
            "step 64, loss: 4.215957, lr:0.0003, norm:1.9421, dt: 21771.92ms, tok/sec: 24080.93\n",
            "step 65, loss: 4.189108, lr:0.0002, norm:4.9333, dt: 21742.16ms, tok/sec: 24113.88\n",
            "step 66, loss: 4.269604, lr:0.0002, norm:3.9644, dt: 21773.24ms, tok/sec: 24079.47\n",
            "step 67, loss: 4.221698, lr:0.0002, norm:2.3273, dt: 21741.96ms, tok/sec: 24114.11\n",
            "step 68, loss: 4.179794, lr:0.0002, norm:0.8209, dt: 21773.05ms, tok/sec: 24079.67\n",
            "step 69, loss: 4.158021, lr:0.0002, norm:1.4846, dt: 21773.48ms, tok/sec: 24079.20\n",
            "step 70, loss: 4.115656, lr:0.0002, norm:0.8959, dt: 21772.38ms, tok/sec: 24080.41\n",
            "step 71, loss: 4.130238, lr:0.0002, norm:1.0777, dt: 21751.65ms, tok/sec: 24103.36\n",
            "step 72, loss: 4.091191, lr:0.0002, norm:0.8562, dt: 21752.81ms, tok/sec: 24102.09\n",
            "step 73, loss: 4.065508, lr:0.0002, norm:2.6671, dt: 21773.02ms, tok/sec: 24079.71\n",
            "step 74, loss: 4.136845, lr:0.0002, norm:5.2008, dt: 21773.65ms, tok/sec: 24079.01\n",
            "step 75, loss: 4.085094, lr:0.0002, norm:2.4182, dt: 21771.28ms, tok/sec: 24081.63\n",
            "step 76, loss: 4.052195, lr:0.0002, norm:1.3825, dt: 21774.76ms, tok/sec: 24077.79\n",
            "step 77, loss: 4.051785, lr:0.0001, norm:2.7789, dt: 21770.93ms, tok/sec: 24082.02\n",
            "step 78, loss: 4.028221, lr:0.0001, norm:1.6737, dt: 21774.38ms, tok/sec: 24078.20\n",
            "step 79, loss: 4.045619, lr:0.0001, norm:1.9383, dt: 21772.08ms, tok/sec: 24080.74\n",
            "step 80, loss: 4.021604, lr:0.0001, norm:1.3186, dt: 21774.52ms, tok/sec: 24078.05\n",
            "step 81, loss: 4.027011, lr:0.0001, norm:1.7302, dt: 21771.51ms, tok/sec: 24081.38\n",
            "step 82, loss: 3.981711, lr:0.0001, norm:1.0818, dt: 21771.48ms, tok/sec: 24081.41\n",
            "step 83, loss: 3.984956, lr:0.0001, norm:1.9284, dt: 21771.74ms, tok/sec: 24081.13\n",
            "step 84, loss: 3.980897, lr:0.0001, norm:0.8864, dt: 21775.03ms, tok/sec: 24077.48\n",
            "step 85, loss: 3.953080, lr:0.0001, norm:1.7302, dt: 21781.17ms, tok/sec: 24070.70\n",
            "step 86, loss: 3.961182, lr:0.0001, norm:0.9044, dt: 21783.67ms, tok/sec: 24067.94\n",
            "step 87, loss: 3.929646, lr:0.0001, norm:1.5691, dt: 21782.37ms, tok/sec: 24069.37\n",
            "step 88, loss: 3.915785, lr:0.0001, norm:1.0405, dt: 21774.27ms, tok/sec: 24078.32\n",
            "step 89, loss: 3.926717, lr:0.0001, norm:0.5799, dt: 21781.90ms, tok/sec: 24069.89\n",
            "step 90, loss: 3.909877, lr:0.0001, norm:0.7646, dt: 21772.10ms, tok/sec: 24080.73\n",
            "step 91, loss: 3.887388, lr:0.0001, norm:0.6448, dt: 21772.77ms, tok/sec: 24079.98\n",
            "step 92, loss: 3.908514, lr:0.0000, norm:0.8466, dt: 21773.09ms, tok/sec: 24079.64\n",
            "step 93, loss: 3.876483, lr:0.0000, norm:0.5985, dt: 21772.77ms, tok/sec: 24079.99\n",
            "step 94, loss: 3.876255, lr:0.0000, norm:0.6740, dt: 21771.93ms, tok/sec: 24080.92\n",
            "step 95, loss: 3.878429, lr:0.0000, norm:0.5089, dt: 21773.09ms, tok/sec: 24079.64\n",
            "step 96, loss: 3.850515, lr:0.0000, norm:0.5347, dt: 21772.68ms, tok/sec: 24080.09\n",
            "step 97, loss: 3.895559, lr:0.0000, norm:0.5564, dt: 21772.28ms, tok/sec: 24080.53\n",
            "step 98, loss: 3.868375, lr:0.0000, norm:0.5064, dt: 21772.71ms, tok/sec: 24080.05\n",
            "step 99, loss: 3.885421, lr:0.0000, norm:0.6340, dt: 21774.48ms, tok/sec: 24078.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "After training I will be running inference on the trained model and I will be fully expecting below average results as I'm not training the model on the entire dataset."
      ],
      "metadata": {
        "id": "2ThUcdmPQ0s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "num_return_sequences = 15\n",
        "max_length = 250\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(\"Question:\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x = tokens.to(device)\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "\n",
        "torch.manual_seed(78)\n",
        "torch.cuda.manual_seed(78)\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(\"Question:\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x = tokens.to(device)\n",
        "while x.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(x)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\"+ decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPhvrJ9DkK-N",
        "outputId": "b1688adb-dfb7-44ec-eb40-c00df38ebae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">Question:\n",
            "'(<a href=\"http://www.j-19_30.j-12.com/2010-archive.jpg\" target=\"_blank\">here</a> 2</a>)  The state in you in the U.com/2009-archive.com/2009-19_DJ_blank\">here</a>'\n",
            "\n",
            "\n",
            "$400\n",
            "\n",
            "\n",
            "Question:\n",
            "'\n",
            "\n",
            "$400\n",
            "\n",
            "\n",
            "A U.jpg\" target=\"http://www.j-01-archive.com/2005-01-archive.wmv\">here</a>'\n",
            "Value:\n",
            "Question:\n",
            "'It's the world in the Clue Crew in the 1 \"The one by the first of the Clue Crewblank\">here</a>'\n",
            "Value:\n",
            "Value:\n",
            "$1,<a href=\"_blank\">He was the Greek in the largest to \"the first the Clue Crew was not over the highest,<br />\" target=\"http://www.jpg\" in a the World\" target=\"http://www.j-archive.com/2005-archive.com/media/v\">here</a> with these in the Cl\n",
            ">Question:\n",
            "$200\n",
            ":\n",
            "'D.Sopon\n",
            "\n",
            "\n",
            "$200\n",
            "\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'In it is over the 2, \"Cah, that has a new American term'\n",
            "$1000\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "'The title of this \"S.S.Cag\"'\n",
            "$200\n",
            "\n",
            "Answer:\n",
            "New Zealand\n",
            "Question:\n",
            "'A name of the last name in the capital was known as her type of a book by a \"All when you'\n",
            "\n",
            "Value:\n",
            "Value:\n",
            "$600\n",
            "Question:\n",
            "$1000\n",
            "\n",
            "\n",
            "Question:\n",
            "'He from a body of the man is the Clue Crew reports with 1.S.   can by the monitor is <a href=\"_17_DJ_blank\">here</a>'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "$1000\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "'You'll're an only first than a 2 to her man that that was a man with a 4, seen here & he in a man on this actor called one'\n",
            "Value:\n",
            "\n",
            "\n",
            "Answer:\n",
            "$1000\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "'\n",
            "'\n",
            ">Question:\n",
            "'H century from which has an \"Tos'\n",
            "$1000\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "'The 2, the only to the U.j-archive.D.j-07-die of the world'\n",
            "\n",
            "$800\n",
            "\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "'From 18-06-02-archive. president, this type of the name of\" in\" target=\"_26_03-05_17.com/2001-09_26.jpg\" target=\"_09_blank\">here</a href=\"_16.mp3\">Sarah of a 4, an \"I.jpg\" target=\"_J_blank\">here</</a href=\"_blank\">here</a>'\"G\\'s of this type of the British is it's a day in a type of an</a>'\n",
            "$800\n",
            "\n",
            "$400\n",
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "$600\n",
            "Question:\n",
            "$800\n",
            "\n",
            "Question:\n",
            "'It's the the head of the Clue Crew like the British day of his title of the same name with me\"'\n",
            "$100\n",
            "\n",
            "Answer:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ">Question:\n",
            "$200\n",
            ":\n",
            "Question:\n",
            "'The 2-word word of any name of this island of a 3-04-archive. Bush's 1 of a largest \"He from the the world's most the one of the title of the first \"T.com/media/media/2004-11-to-03-01-foot-02-05-23_DJ_blank\">this</a href=\"_28.jpg\" target=\"http://www.j-02_28_J_blank\">Sarah of this 3s,<br />\" target=\"_17a>) </a> <a name, he on a time of the Clue Crew for the capital'\n",
            "$1000\n",
            "\n",
            "\n",
            "Question:\n",
            "$200\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "'\n",
            "\n",
            "Value:\n",
            "Question:\n",
            "'H600\n",
            "Value:\n",
            "'The Greek for New York\n",
            "$2's first film in this river of the only the U.C.com/2007-04. Louis)\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "Answer:\n",
            "Nes,000\n",
            "Question:\n",
            "'\n",
            "$400\n",
            "\n",
            "\n",
            "Question:\n",
            "'This man's the other man's\n",
            ">Question:\n",
            "'In you're its the same man on water'\n",
            "Value:\n",
            "$800\n",
            "\n",
            "Answer:\n",
            "\n",
            "K.wm2005-03-foot-27_25_26_blank\">here</a>'s the World,000\n",
            "Answer:\n",
            "'You</a>) The the New C\" target=\"http://www.com/2010-archive.j-01-27_blank\">The first first type of the name'\n",
            "\n",
            "\n",
            "Value:\n",
            "$400\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'It wrote of the Clue Crew that the U.com/2009-11-archive.jpg\" target=\"_DJ_blank\">here</a href=\"_blank\">here</a>, I've was at a man in this'\n",
            "Value:\n",
            "$200\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "'One of a word was a a this title,200\n",
            "$500\n",
            "\n",
            "Answer:\n",
            "Perta.jpg\" target=\"_blank\">here</a>'\n",
            "Answer:\n",
            "(<br />G, which the Clue Crew can be a 1820 in the \"D.'\n",
            "Question:\n",
            "'Gin is the name'\n",
            "Value:\n",
            ">Question:\n",
            "'A the the 2 of this year & called the world in his famous for \"Gam-07th century word for a way & you the name'\n",
            "\n",
            "$800\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "'The Black\n",
            "\n",
            "\n",
            "Question:\n",
            "'The Clue Crew're make your American man, also one of these'\n",
            "\n",
            "$1000\n",
            "\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "$800\n",
            "Question:\n",
            "'This \"The name of these city in his largests for them to the last name of these'The most than this song, 18old)'\n",
            "Value:\n",
            "$1000\n",
            "Answer:\n",
            "\n",
            "a\n",
            "Question:\n",
            "'K.j-22.  G.jpg\" was the name of the the New film'\n",
            "Value:\n",
            "\n",
            "\n",
            "Value:\n",
            "Value:\n",
            "$200\n",
            "\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'In 17 Clue Crew can have the world's most time was the U.wms who died to be to't \"The U.Ler with a famous for a part of the Great\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'Born in this state & a play out of the title'He's\n",
            ">Question:\n",
            "'\"My\n",
            "Value:\n",
            "'\"the:\n",
            "'Cum & this actor, I in 1806-03-20-archive.jpg\" target=\"http://www.com/media/2004-01-09-30. a> is the \"The second, but by an the first of this TV, the Clue Crew of a great to be to his'\n",
            "\n",
            "$1200\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "$100\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'This country,600\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "'Its name to the 1719-archive.S.j-archive.mps'This \"A-archive.j-archive.com/media/media/media/media/media/media/2005-archive.jpg\" target=\"_J_08.jpg\" target=\"_blank\">here</a>) is from the 2 show'\n",
            "Value:\n",
            "$2000\n",
            "Answer:\n",
            "Question:\n",
            "'This name of this many \"The 5-d_DJ_J_J_DJ_blank\"> of the Clue Crew, the British, was a real 1, including a little book,000\n",
            "$800\n",
            "Answer\n",
            ">Question:\n",
            "'\"thewww.C.',<a> as more-07-29-archive.j-archive.com/media/2005-09-21_29_DJ_DJ_blank\">/media/2008-archive.jpg\" target=\"_J_blank\">this</a> this</a>)  the Clue Crew of my <a>'s the world's the the first of the Clue Crew're named for 3-archive.com/2008-01-archive.j-21.S.jpg\" target=\"_27_blank\">here</a.j-10-archive.com/media/media/2009-archive.j-archive.jpg\" target=\"_DJ_blank\">here</a>'\n",
            "$1600\n",
            "\n",
            "Value:\n",
            "\n",
            "\n",
            "$Djpg\" target=\"_blank\">here</a>'\n",
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "a> to the Clue Crew have by a most of the clue in \"The Uniteda\n",
            "P-archive.j-archive.jpg\" target=\"_blank\">here</a href=\"_Value:\n",
            "'In 1808.jpg\"Question:\n",
            "'The\n",
            ">Question:\n",
            "$200\n",
            ":\n",
            "\n",
            "$2000\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "'He's other & \"the Lab of the word one of the first'\n",
            "\n",
            "\n",
            "\n",
            "Value:\n",
            "$800\n",
            "P\n",
            "\n",
            "Answer:\n",
            "\n",
            "\n",
            "Question:\n",
            "'The first play seen <a href=\"_26_J.com/2010-archive.S.com/2001-07-23.jpg\" target=\"_15.jpg\" target=\"_DJ_DJ_blank\">here</a> as I'm the <a> in the name from it had its name about a own city\n",
            "$600\n",
            "\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'A term for well show, a 2 \"The British title of their state to be get by this game'\n",
            "Value:\n",
            "$200\n",
            "Answer:\n",
            "\n",
            "\n",
            "Question:\n",
            "'\n",
            "$800\n",
            "Answer:\n",
            "pahi>'In these term for the Clue Crew also a \"B.'\n",
            "Answer:\n",
            "Answer:\n",
            "Question:\n",
            "'It's play, the Clue Crew reports on this word in a famous be in its Latin for this, it'\n",
            "\n",
            "\n",
            "\n",
            "$800\n",
            "Answer:\n",
            "E-\n",
            ">Question:\n",
            "$500\n",
            ":\n",
            "'\n",
            "\n",
            "\n",
            "Value:\n",
            "'It)\n",
            "\n",
            "\n",
            "$1200\n",
            "Answer:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "'Foous of the 2 name for't the 'or\n",
            "Value:\n",
            "$2000\n",
            "Answer:\n",
            "\n",
            "S.j-27.Duget\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "'This 3, he are the Clue Crew for this French this group \"The first'She's a film that as this type of his world-archive. Bush'\n",
            "\n",
            "\n",
            "\n",
            "Value:\n",
            "$600\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "'In the Great, not in 14-04'\n",
            "\n",
            "\n",
            "$1000\n",
            "Answer:\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'The first country'\n",
            "Value:\n",
            "'\"(<br />\" target=\"_18. Louis-archive.jpg\"</a> who in 17'm have be an capital's to know one.</a>'\n",
            "Value:\n",
            "$800\n",
            "\n",
            "Value:\n",
            "Answer:\n",
            "Question:\n",
            "'He was to this'\n",
            "Value:\n",
            "$1000\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'This 19 of 2\n",
            ">Question:\n",
            "'The French one to in this novel in this film in a largest'\n",
            "Value:\n",
            "$400\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "C:\n",
            "Question:\n",
            "'In its name of these \"a href=\"http://www.j-archive.j-04-11-letter name from a word for a large day'\n",
            "$600\n",
            "\n",
            "\n",
            "\n",
            "Value:\n",
            "Question:\n",
            "'C. the U.com/2009-11-17_DJ_J_blank\">here</ (or.</a href=\"_DJ_J_DJ_blank\">Jimmy of the Nationalo\n",
            "$1000\n",
            "Answer:\n",
            "\n",
            "E.jpg\" target=\"_blank\">Jimmy of 12-12_blank\">here</a\n",
            "\n",
            "$1000\n",
            "Answer:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "the P_DJ_J_blank\">Sarah of her,000\n",
            "\n",
            "Question:\n",
            "'This TV to the name'The Latin for this first last to the world's the Unitedas,000\n",
            "\n",
            "\n",
            "\n",
            "$200\n",
            "Answer:\n",
            "\n",
            "Answer:\n",
            "a>'I</a>)  The first used for New years, \"E.'\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "'\n",
            ">Question:\n",
            "$800\n",
            ":\n",
            "'\"This \"The Su\n",
            "\n",
            "Answer:\n",
            "'Question:\n",
            "'In this country's the year, the first \"All the \"The monitor.</a href=\"http://www.com/media/media/media/media/media/media/2006-11-04-26_DJ_J_26.jpg\" target=\"_24.jpg\" target=\"_27.jpg\" target=\"_blank\">here</a href=\"_blank\">seen</i>'s \"the Him'\n",
            "\n",
            "Value:\n",
            "$400\n",
            "\n",
            "Question:\n",
            "$600\n",
            "Question:\n",
            "'The the 3-archive.wm2009-03-archive.j-archive.com/media/media/media/media/media/2007-archive.jpg\" target=\"_blank\">Jimmy of the 4's one of its to do on a man a name this \"The one of the state in the U.S.jpg\" this man.</a>)  I is the Cl target=\"_blank\">here</a href=\"_DJ_blank\">here</a>'\n",
            "Value:\n",
            "Value:\n",
            "$1000\n",
            "Answer:\n",
            "\n",
            "\n",
            "a-\n",
            ">Question:\n",
            "'The British name of the name when I for Newold in this \"The the British group; known after to the largest'\n",
            "Value:\n",
            "Value:\n",
            "\n",
            "$800\n",
            "Answer:\n",
            "Question:\n",
            "'This river of this capital of this \"A word for this state, \"The Great is a new name'\n",
            "Value:\n",
            "\n",
            "$2000\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "'This country in the '76 is a largest name of it was the U. by a American for this TV this TV, which & the \"Len-word film'\n",
            "\n",
            "Value:\n",
            "\n",
            "Value:\n",
            "\n",
            "\n",
            "$800\n",
            "\n",
            "Answer:\n",
            "Question:\n",
            "'\n",
            "$1000\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "'The British line & a French woman in this state's \"The first-archive.j-archive.S.com/media/media/2006 in this \"S.N.S. a country to a name when which the first name for \"The Great & one of <a>'\n",
            "\n",
            "\n",
            "\n",
            "Value:\n",
            "Value:\n",
            "Value:\n",
            "$600\n",
            "Answer:\n",
            "Answer:\n",
            "Question:\n",
            "'In this man on its U.S.j-word\n",
            ">Question:\n",
            "'The 5'\n",
            "\n",
            "$400 was the 'I-letter first 2, the first company played 2-archive.C. of the \"a> city has been be an 2; a \"D.''\n",
            "\n",
            "$200\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "\n",
            "Question:\n",
            "'This \"A \"The '/2007-archive.jpg\" target=\"_J_blank\">\"\n",
            "$800\n",
            "Answer:\n",
            "Peth\n",
            "\n",
            "\" target=\"http://www.j-archive.j-02_blank\">Jimmy of this country's \"J_15.com/2004-archive.jpg\" target=\"_J_blank\">this</a> are been the name \"All</a>'\n",
            "Answer:\n",
            "\n",
            "Answer:\n",
            "\n",
            "\n",
            "\n",
            "Value:\n",
            "\n",
            "$1000\n",
            "the tb.jpg\" target=\"_DJ_DJ_11-archive.jpg\" target=\"http://www.com/2008-23_DJ_J_blank\">here</a. Louis\"\n",
            "Value:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "$1000\n",
            "\n",
            "Question:\n",
            "\n",
            "Question:\n",
            "'The U.jpg\" target=\"_27_26.com/2004\n",
            ">Question:\n",
            "'\"Pwww.j-old the body in the title of this man, which's ' largest of 3 that with one'\n",
            "\n",
            "$2000\n",
            "\n",
            "Value:\n",
            "\n",
            "the America\n",
            "Question:\n",
            "'The most first line's \"The Greek & this river'\n",
            "Value:\n",
            "$1200\n",
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "a\n",
            "St.C.'\n",
            "\"This word can be a world was a own word called \"$300\n",
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "Question:\n",
            "'I'm the father was the title, this country, but \"to a state's 2-archive.com/media/media/media/media/media/media/media/2007-archive.jpg\" target=\"_-star-11-archive.S.jpg\" target=\"_DJ_J_03_blank\">here</a>)  In <a href=\"http://www.jpg\" target=\"_blank\">here</a> is the 3 in the Clue Crew as <br />Value:\n",
            "$400\n",
            "Answer:\n",
            "Question:\n",
            "'A-12-02_blank\">Jimmy of the first first of the name is a film a bird'\n",
            "$400\n",
            "\n",
            "Value\n"
          ]
        }
      ]
    }
  ]
}