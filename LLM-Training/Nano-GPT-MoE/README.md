In this folder I will be including a notebook where I will be building GPT-2 from scratch and modifying it's architecture to include Mixture of Experts(MoE) from 
the "A Review of Sparse Expert Models in Deep Learning research" research paper(https://arxiv.org/abs/2209.01667) by William Fedus, Jeff Dean, Barret Zoph.
